name: Performance Testing & Monitoring
# Repository: https://github.com/dagiim/webops
# Owner: Douglas Mutethia (Eleso Solutions)
# WebOps: Self-hosted VPS hosting platform for deploying and managing web applications

on:
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  push:
    branches: [ main, develop ]
    paths:
      - 'control-panel/**'
      - 'apps/deployments/**'
      - 'apps/core/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'control-panel/**'
      - 'apps/deployments/**'
      - 'apps/core/**'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - api
          - database
          - load
          - stress

env:
  BASE_URL: http://localhost:8000
  TEST_DB_PATH: /tmp/test_performance.db

jobs:
  performance-baseline:
    name: Performance Baseline Tests
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: webops_perf
          POSTGRES_PASSWORD: perfpassword
          POSTGRES_DB: webops_perf
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'control-panel/requirements.txt'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client redis-tools nginx

    - name: Install Python dependencies
      run: |
        cd control-panel
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-django pytest-benchmark locust django-debug-toolbar django-silk

    - name: Create performance test environment
      run: |
        cd control-panel
        cat > .env << EOF
        DEBUG=False
        SECRET_KEY=perf-test-secret-key-${{ github.sha }}
        DATABASE_URL=postgresql://webops_perf:perfpassword@localhost:5432/webops_perf
        CELERY_BROKER_URL=redis://localhost:6379/0
        CELERY_RESULT_BACKEND=redis://localhost:6379/1
        ALLOWED_HOSTS=*
        ENCRYPTION_KEY=$(python -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())')
        WEBOPS_ENVIRONMENT=performance
        PERFORMANCE_MODE=True
        EOF

    - name: Run database migrations
      run: |
        cd control-panel
        python manage.py migrate --noinput

    - name: Create test data
      run: |
        cd control-panel
        python manage.py shell << EOF
        from django.contrib.auth.models import User
        from apps.deployments.models import Deployment, DeploymentLog
        from apps.core.models import UserPreferences
        
        # Create test users
        for i in range(10):
            User.objects.create_user(f'perfuser{i}', f'perf{i}@test.com', 'testpass')
        
        # Create test deployments
        for i in range(20):
            Deployment.objects.create(
                name=f'test-deployment-{i}',
                repository_url='https://github.com/test/repo.git',
                branch='main',
                status='running',
                port=8000 + i
            )
        
        print("Test data created successfully")
        EOF

    - name: Collect static files
      run: |
        cd control-panel
        python manage.py collectstatic --noinput

    - name: Start application
      run: |
        cd control-panel
        python manage.py runserver 0.0.0.0:8000 &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        sleep 15
        curl -f http://localhost:8000/ || exit 1

    - name: Run API performance tests
      run: |
        echo "üß™ Running API performance tests"
        cd control-panel
        
        python manage.py test apps.deployments.tests.test_performance --verbosity=2 || true
        
        # Test specific endpoints
        echo "Testing API endpoint performance..."
        
        # Time various API calls
        {
          echo "=== API Performance Results ==="
          echo "Deployment list API:"
          time curl -s http://localhost:8000/api/deployments/ > /dev/null
          
          echo "User list API:"
          time curl -s http://localhost:8000/api/users/ > /dev/null
          
          echo "Dashboard API:"
          time curl -s http://localhost:8000/api/dashboard/ > /dev/null
        } >> performance-results.txt

    - name: Run database performance tests
      run: |
        echo "üóÑÔ∏è Running database performance tests"
        cd control-panel
        
        python manage.py shell << EOF
        import time
        import django
        django.setup()
        
        from apps.deployments.models import Deployment
        from django.db import connection
        
        print("=== Database Performance Results ===")
        
        # Test query performance
        queries = [
            ("All deployments", lambda: list(Deployment.objects.all())),
            ("Active deployments", lambda: list(Deployment.objects.filter(status='running'))),
            ("Deployment count", lambda: Deployment.objects.count()),
        ]
        
        for name, query_func in queries:
            start_time = time.time()
            result = query_func()
            end_time = time.time()
            duration = end_time - start_time
            print(f"{name}: {duration:.3f}s ({len(result)} results)")
        
        # Check database connection
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
            print("Database connection: OK")
        EOF

    - name: Run load tests with Locust
      run: |
        echo "üöÄ Running load tests with Locust"
        
        # Create Locust test file
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        
        class WebOpsUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                """Login on start"""
                response = self.client.post("/api/auth/login/", {
                    "username": "perfuser0",
                    "password": "testpass"
                })
                if response.status_code == 200:
                    self.token = response.json().get("token")
                else:
                    self.token = None
            
            @task(3)
            def view_dashboard(self):
                """View dashboard"""
                headers = {"Authorization": f"Token {self.token}"} if self.token else {}
                self.client.get("/api/dashboard/", headers=headers)
            
            @task(2)
            def list_deployments(self):
                """List deployments"""
                headers = {"Authorization": f"Token {self.token}"} if self.token else {}
                self.client.get("/api/deployments/", headers=headers)
            
            @task(1)
            def create_deployment(self):
                """Create deployment (test endpoint)"""
                headers = {"Authorization": f"Token {self.token}"} if self.token else {}
                self.client.post("/api/deployments/", {
                    "name": "perf-test-deployment",
                    "repository_url": "https://github.com/test/repo.git",
                    "branch": "main"
                }, headers=headers, catch_response=True)
        EOF
        
        # Run locust tests
        locust -f locustfile.py --headless --users 10 --spawn-rate 2 --run-time 60s --host http://localhost:8000 --csv=load-test-results || true

    - name: Run memory profiling
      run: |
        echo "üíæ Running memory profiling"
        cd control-panel
        
        pip install memory-profiler psutil
        
        # Create memory profiling script
        cat > memory_profile.py << 'EOF'
        import os
        os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
        import django
        django.setup()
        
        from memory_profiler import profile
        from apps.deployments.models import Deployment
        
        @profile
        def test_deployment_queries():
            # Test memory usage of various queries
            deployments = Deployment.objects.all()
            for deployment in deployments:
                _ = deployment.name
                _ = deployment.status
                _ = deployment.created_at
        
        if __name__ == "__main__":
            test_deployment_queries()
        EOF
        
        python memory_profile.py > memory-profile-results.txt 2>&1 || true

    - name: Run benchmark tests
      run: |
        echo "‚ö° Running benchmark tests"
        cd control-panel
        
        cat > test_benchmarks.py << 'EOF'
        import pytest
        import time
        from django.test import Client
        from django.contrib.auth.models import User
        from apps.deployments.models import Deployment
        
        @pytest.mark.django_db
        def test_deployment_list_performance(benchmark):
            client = Client()
            user = User.objects.create_user('benchmark', 'test@test.com', 'pass')
            client.force_login(user)
            
            def deployment_list():
                return client.get('/api/deployments/')
            
            result = benchmark(deployment_list)
            assert result.status_code == 200
        
        @pytest.mark.django_db
        def test_deployment_creation_performance(benchmark):
            client = Client()
            user = User.objects.create_user('benchmark2', 'test2@test.com', 'pass')
            client.force_login(user)
            
            def create_deployment():
                return client.post('/api/deployments/', {
                    'name': 'benchmark-deployment',
                    'repository_url': 'https://github.com/test/repo.git',
                    'branch': 'main'
                })
            
            result = benchmark(create_deployment)
            assert result.status_code in [200, 201]
        EOF
        
        python -m pytest test_benchmarks.py --benchmark-only || true

    - name: Generate performance report
      run: |
        echo "# Performance Test Report - $(date)" > performance-report.md
        echo "" >> performance-report.md
        echo "**Repository:** ${{ github.repository }}" >> performance-report.md
        echo "**Commit:** ${{ github.sha }}" >> performance-report.md
        echo "**Test Type:** ${{ github.event.inputs.test_type || 'scheduled' }}" >> performance-report.md
        echo "" >> performance-report.md
        
        echo "## API Performance" >> performance-report.md
        if [ -f control-panel/performance-results.txt ]; then
          cat control-panel/performance-results.txt >> performance-report.md
        fi
        
        echo "" >> performance-report.md
        echo "## Load Test Results" >> performance-report.md
        if [ -f load-test-results_stats.csv ]; then
          echo "Load test statistics saved to CSV file" >> performance-report.md
        fi
        
        echo "" >> performance-report.md
        echo "## Memory Profile" >> performance-report.md
        if [ -f control-panel/memory-profile-results.txt ]; then
          echo "Memory profiling completed (see artifacts for full results)" >> performance-report.md
        fi
        
        echo "" >> performance-report.md
        echo "## Performance Recommendations" >> performance-report.md
        echo "" >> performance-report.md
        echo "1. Monitor database query performance" >> performance-report.md
        echo "2. Implement database query optimization" >> performance-report.md
        echo "3. Consider caching frequently accessed data" >> performance-report.md
        echo "4. Monitor memory usage in production" >> performance-report.md
        echo "5. Implement rate limiting for API endpoints" >> performance-report.md
        
        cat performance-report.md

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      with:
        name: performance-report-${{ github.run_number }}
        path: |
          performance-report.md
          control-panel/performance-results.txt
          load-test-results_*.csv
          control-panel/memory-profile-results.txt

    - name: Performance regression check
      run: |
        echo "üîç Checking for performance regressions"
        
        # Download previous performance report
        if [ -n "${{ github.event.inputs.test_type }}" ]; then
          echo "Skipping regression check for manual runs"
        else
          echo "Would check against previous performance metrics"
          # Add regression detection logic here
        fi

  stress-testing:
    name: Stress Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'stress' || github.event.inputs.test_type == 'full'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install stress testing tools
      run: |
        pip install locust
        sudo apt-get update
        sudo apt-get install -y stress

    - name: Start test application
      run: |
        cd control-panel
        cat > .env << EOF
        DEBUG=False
        SECRET_KEY=stress-test-secret-key
        DATABASE_URL=sqlite://${{ env.TEST_DB_PATH }}
        ALLOWED_HOSTS=*
        EOF
        
        python manage.py runserver 0.0.0.0:8000 &
        echo "APP_PID=$!" >> $GITHUB_ENV
        sleep 10

    - name: Run stress tests
      run: |
        echo "üí™ Running stress tests"
        
        # Create stress test configuration
        cat > stress_locust.py << 'EOF'
        from locust import HttpUser, task, between
        
        class StressUser(HttpUser):
            wait_time = between(0.1, 0.5)  # Very fast requests
            
            @task(5)
            def rapid_requests(self):
                self.client.get("/api/deployments/")
            
            @task(3)
            def concurrent_operations(self):
                self.client.post("/api/deployments/", {
                    "name": "stress-test",
                    "repository_url": "https://github.com/test/repo.git"
                })
            
            @task(2)
            def heavy_operations(self):
                self.client.get("/api/dashboard/")
        EOF
        
        # Run stress test
        locust -f stress_locust.py --headless --users 50 --spawn-rate 10 --run-time 120s --host http://localhost:8000 || true

    - name: Monitor system resources
      run: |
        echo "üìä Monitoring system resources during stress test"
        
        # Monitor CPU and memory usage
        {
          echo "=== System Resource Usage ==="
          echo "CPU usage:"
          top -bn1 | grep "Cpu(s)" || echo "CPU monitoring not available"
          
          echo "Memory usage:"
          free -h || echo "Memory monitoring not available"
          
          echo "Disk usage:"
          df -h / || echo "Disk monitoring not available"
        } >> stress-test-results.txt

    - name: Upload stress test results
      uses: actions/upload-artifact@v4
      with:
        name: stress-test-results
        path: stress-test-results.txt

  database-performance:
    name: Database Performance Analysis
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'database' || github.event.inputs.test_type == 'full'

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: webops_db_perf
          POSTGRES_PASSWORD: dbperfpassword
          POSTGRES_DB: webops_db_perf
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install database performance tools
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client
        pip install django-debug-toolbar django-silk psycopg2-binary

    - name: Set up test database
      run: |
        cd control-panel
        cat > .env << EOF
        DEBUG=True
        SECRET_KEY=db-perf-test-key
        DATABASE_URL=postgresql://webops_db_perf:dbperfpassword@localhost:5432/webops_db_perf
        ALLOWED_HOSTS=*
        EOF
        
        python manage.py migrate --noinput

    - name: Create large test dataset
      run: |
        cd control-panel
        python manage.py shell << EOF
        from apps.deployments.models import Deployment, DeploymentLog
        from django.contrib.auth.models import User
        
        # Create test users
        users = [User.objects.create_user(f'user{i}', f'user{i}@test.com', 'pass') for i in range(50)]
        
        # Create large number of deployments
        deployments = []
        for i in range(1000):
            deployment = Deployment.objects.create(
                name=f'perf-test-deployment-{i}',
                repository_url='https://github.com/test/repo.git',
                branch='main',
                status='running' if i % 2 == 0 else 'stopped',
                port=8000 + i
            )
            deployments.append(deployment)
            
            # Create logs for each deployment
            for j in range(10):
                DeploymentLog.objects.create(
                    deployment=deployment,
                    level='INFO',
                    message=f'Test log message {j} for deployment {i}'
                )
        
        print(f"Created {Deployment.objects.count()} deployments")
        print(f"Created {DeploymentLog.objects.count()} deployment logs")
        EOF

    - name: Run database query analysis
      run: |
        cd control-panel
        python manage.py shell << EOF
        import django
        django.setup()
        
        from django.db import connection
        from django.db.models import Count
        
        print("=== Database Query Performance Analysis ===")
        
        # Analyze query performance
        queries = [
            ("Deployments with logs", 
             lambda: list(Deployment.objects.annotate(log_count=Count('logs')).filter(log_count__gt=0))),
            ("Active deployments count", 
             lambda: Deployment.objects.filter(status='running').count()),
            ("Recent deployments", 
             lambda: list(Deployment.objects.filter(created_at__gte=django.utils.timezone.now().date() - django.utils.timezone.timedelta(days=30)))),
        ]
        
        for name, query_func in queries:
            import time
            start_time = time.time()
            result = query_func()
            end_time = time.time()
            
            # Enable query logging for analysis
            from django.conf import settings
            settings.DEBUG = True
            
            print(f"\n{name}:")
            print(f"  Results: {len(result)}")
            print(f"  Time: {end_time - start_time:.3f}s")
            print(f"  Queries: {len(connection.queries)}")
            
            # Reset for next query
            connection.queries_log.clear()
        
        # Check for N+1 query problems
        print("\n=== N+1 Query Detection ===")
        settings.DEBUG = True
        
        # This should trigger N+1 if not properly optimized
        deployments = Deployment.objects.all()[:10]
        for deployment in deployments:
            _ = deployment.logs.count()
        
        print(f"Queries executed: {len(connection.queries)}")
        if len(connection.queries) > 15:  # More than expected for 10 deployments
            print("‚ö†Ô∏è Potential N+1 query problem detected!")
        EOF

    - name: Check database indexes
      run: |
        echo "=== Database Index Analysis ==="
        
        PGPASSWORD=dbperfpassword psql -h localhost -U webops_db_perf -d webops_db_perf << EOF
        -- Check index usage
        SELECT 
            schemaname,
            tablename,
            indexname,
            indexdef
        FROM pg_indexes 
        WHERE schemaname = 'public'
        ORDER BY tablename, indexname;
        
        -- Check for missing indexes on foreign keys
        SELECT
            tc.table_name, 
            kcu.column_name, 
            ccu.table_name AS foreign_table_name,
            ccu.column_name AS foreign_column_name 
        FROM 
            information_schema.table_constraints AS tc 
            JOIN information_schema.key_column_usage AS kcu
              ON tc.constraint_name = kcu.constraint_name
              AND tc.table_schema = kcu.table_schema
            JOIN information_schema.constraint_column_usage AS ccu
              ON ccu.constraint_name = tc.constraint_name
              AND ccu.table_schema = tc.table_schema
        WHERE tc.constraint_type = 'FOREIGN KEY' 
          AND tc.table_schema='public';
        EOF

    - name: Upload database analysis results
      uses: actions/upload-artifact@v4
      with:
        name: database-performance-analysis
        path: |
          control-panel/
          database-analysis.txt

  performance-monitoring:
    name: Performance Monitoring Setup
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'full'

    steps:
    - name: Setup performance monitoring
      run: |
        echo "üîß Setting up performance monitoring configuration"
        
        # Create monitoring configuration
        cat > performance-monitoring.md << 'EOF'
        # Performance Monitoring Configuration
        
        ## Key Metrics to Monitor
        
        ### Application Performance
        - Response time (API endpoints)
        - Throughput (requests per second)
        - Error rate
        - Database query time
        - Memory usage
        - CPU utilization
        
        ### Infrastructure Performance
        - PostgreSQL performance
        - Redis performance
        - Celery queue performance
        - Disk I/O
        - Network latency
        
        ### Monitoring Tools
        - Django Debug Toolbar (development)
        - Django Silk (profiling)
        - Locust (load testing)
        - Prometheus + Grafana (production)
        - New Relic / DataDog (APM)
        
        ## Performance Baselines
        - API response time: < 200ms (p95)
        - Database queries: < 50ms (average)
        - Page load time: < 2s
        - Memory usage: < 512MB
        - CPU usage: < 70%
        EOF
        
        echo "‚úÖ Performance monitoring configuration created"

    - name: Create performance alerts
      run: |
        echo "üö® Creating performance alert configuration"
        
        cat > performance-alerts.yml << 'EOF'
        # Performance Alert Configuration
        alerts:
          - name: High API Response Time
            condition: api_response_time > 500ms
            duration: 5m
            severity: warning
            
          - name: High Database Query Time
            condition: db_query_time > 100ms
            duration: 2m
            severity: critical
            
          - name: High Memory Usage
            condition: memory_usage > 80%
            duration: 10m
            severity: warning
            
          - name: High CPU Usage
            condition: cpu_usage > 90%
            duration: 5m
            severity: critical
            
          - name: Error Rate Spike
            condition: error_rate > 5%
            duration: 1m
            severity: critical
        EOF
        
        echo "‚úÖ Performance alerts configured"

    - name: Upload monitoring setup
      uses: actions/upload-artifact@v4
      with:
        name: performance-monitoring-setup
        path: |
          performance-monitoring.md
          performance-alerts.yml

  performance-summary:
    name: Performance Test Summary
    runs-on: ubuntu-latest
    needs: [performance-baseline, stress-testing, database-performance]
    if: always()

    steps:
    - name: Create performance summary
      run: |
        echo "# Performance Test Summary - $(date)" > performance-summary.md
        echo "" >> performance-summary.md
        echo "**Repository:** ${{ github.repository }}" >> performance-summary.md
        echo "**Run ID:** ${{ github.run_id }}" >> performance-summary.md
        echo "" >> performance-summary.md
        
        echo "## Test Results Summary" >> performance-summary.md
        echo "" >> performance-summary.md
        
        if [ "${{ needs.performance-baseline.result }}" == "success" ]; then
          echo "‚úÖ **Baseline Performance Tests:** PASSED" >> performance-summary.md
        else
          echo "‚ùå **Baseline Performance Tests:** FAILED" >> performance-summary.md
        fi
        
        if [ "${{ needs.stress-testing.result }}" == "success" ]; then
          echo "‚úÖ **Stress Testing:** PASSED" >> performance-summary.md
        elif [ "${{ needs.stress-testing.result }}" == "skipped" ]; then
          echo "‚è≠Ô∏è **Stress Testing:** SKIPPED" >> performance-summary.md
        else
          echo "‚ùå **Stress Testing:** FAILED" >> performance-summary.md
        fi
        
        if [ "${{ needs.database-performance.result }}" == "success" ]; then
          echo "‚úÖ **Database Performance Analysis:** PASSED" >> performance-summary.md
        elif [ "${{ needs.database-performance.result }}" == "skipped" ]; then
          echo "‚è≠Ô∏è **Database Performance Analysis:** SKIPPED" >> performance-summary.md
        else
          echo "‚ùå **Database Performance Analysis:** FAILED" >> performance-summary.md
        fi
        
        echo "" >> performance-summary.md
        echo "## Performance Recommendations" >> performance-summary.md
        echo "" >> performance-summary.md
        echo "1. **Database Optimization:** Review slow queries and add appropriate indexes" >> performance-summary.md
        echo "2. **Caching Strategy:** Implement Redis caching for frequently accessed data" >> performance-summary.md
        echo "3. **API Rate Limiting:** Add rate limiting to prevent abuse" >> performance-summary.md
        echo "4. **Memory Management:** Monitor memory usage and optimize data structures" >> performance-summary.md
        echo "5. **Load Balancing:** Consider horizontal scaling for high traffic" >> performance-summary.md
        
        cat performance-summary.md

    - name: Upload performance summary
      uses: actions/upload-artifact@v4
      with:
        name: performance-summary
        path: performance-summary.md