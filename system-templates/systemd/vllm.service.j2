[Unit]
Description=vLLM LLM Model Server - {{ app_name }}
After=network.target

[Service]
Type=simple
User={{ webops_user }}
WorkingDirectory={{ venv_path }}
Environment="PATH={{ venv_path }}/bin"
Environment="HF_HOME={{ model_cache_path }}"
Environment="TRANSFORMERS_CACHE={{ model_cache_path }}"
Environment="CUDA_VISIBLE_DEVICES=0"

# vLLM server command
ExecStart={{ python_path }} -m vllm.entrypoints.openai.api_server {{ vllm_args }}

# Logging
StandardOutput=append:{{ log_path }}/vllm.log
StandardError=append:{{ log_path }}/vllm-error.log

# Restart policy
Restart=on-failure
RestartSec=10s

# Resource limits
# vLLM can use significant GPU memory
LimitNOFILE=65536
LimitNPROC=4096

[Install]
WantedBy=multi-user.target
