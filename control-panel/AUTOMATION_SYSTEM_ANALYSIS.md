# ğŸ¤– WebOps Automation System - Comprehensive Analysis

## Overview

WebOps features a **powerful visual workflow automation system** that enables users to create complex automation pipelines with a drag-and-drop interface. Think **Zapier meets n8n** - but integrated directly into your deployment platform!

**Built**: Enterprise-grade workflow orchestration with AI agent integration

**Core Philosophy**: No-code/low-code automation for DevOps, data processing, content generation, and monitoring tasks.

---

## ğŸ¯ System Architecture

### High-Level Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Visual Builder (UI)                   â”‚
â”‚              Drag-and-drop workflow designer            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Workflow Engine                         â”‚
â”‚   - Topological sorting                                  â”‚
â”‚   - Node execution orchestration                         â”‚
â”‚   - Data flow management                                 â”‚
â”‚   - Error handling & retry logic                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Node Executors                           â”‚
â”‚   Data Sources â”‚ Processors â”‚ Outputs â”‚ Control Flow    â”‚
â”‚   - Google Docs â”‚ - LLM     â”‚ - Email â”‚ - Conditions    â”‚
â”‚   - APIs        â”‚ - Transformâ”‚ - Webhooksâ”‚ - Loops      â”‚
â”‚   - Databases   â”‚ - Filter  â”‚ - Databaseâ”‚ - Delays      â”‚
â”‚   - Webhooks    â”‚ - Agent AIâ”‚ - Slack  â”‚ - Error Hdlr  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Agent Bridge (Optional)                     â”‚
â”‚   Connects to AI agents in .webops/agents/              â”‚
â”‚   - Task execution                                       â”‚
â”‚   - Query processing                                     â”‚
â”‚   - Memory management                                    â”‚
â”‚   - Decision making                                      â”‚
â”‚   - Learning feedback                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Database Schema

### Core Models

#### 1. **Workflow**

The main workflow container:

```python
class Workflow(BaseModel):
    name = CharField(max_length=255)
    description = TextField(blank=True)
    owner = ForeignKey(User)

    # Status
    status = CharField(choices=['draft', 'active', 'paused', 'disabled'])
    trigger_type = CharField(choices=['manual', 'schedule', 'webhook', 'event'])

    # Schedule (for scheduled workflows)
    schedule_cron = CharField(max_length=100)  # Cron expression

    # Visual canvas
    canvas_data = JSONField()  # Stores visual state

    # Statistics
    total_executions = IntegerField(default=0)
    successful_executions = IntegerField(default=0)
    failed_executions = IntegerField(default=0)
    last_executed_at = DateTimeField(null=True)
    average_duration_ms = IntegerField(default=0)

    # Settings
    timeout_seconds = IntegerField(default=300)
    retry_on_failure = BooleanField(default=True)
    max_retries = IntegerField(default=3)
```

**Statistics Tracking**: Atomic updates with `F()` expressions to prevent race conditions!

#### 2. **WorkflowNode**

Individual nodes in the workflow graph:

```python
class WorkflowNode(BaseModel):
    workflow = ForeignKey(Workflow)
    node_id = CharField(max_length=100)  # Unique within workflow
    node_type = CharField(max_length=50)  # From NodeType choices
    label = CharField(max_length=255)

    # Visual position
    position_x = IntegerField(default=0)
    position_y = IntegerField(default=0)

    # Configuration
    config = JSONField()  # Node-specific settings

    # Addon integration
    addon = ForeignKey('addons.Addon', null=True)

    # Execution settings
    enabled = BooleanField(default=True)
    timeout_seconds = IntegerField(default=60)
    retry_on_failure = BooleanField(default=False)
    max_retries = IntegerField(default=1)
```

**31 Node Types** supported across 4 categories!

#### 3. **WorkflowConnection**

Edges connecting nodes (directed graph):

```python
class WorkflowConnection(BaseModel):
    workflow = ForeignKey(Workflow)
    source_node = ForeignKey(WorkflowNode, related_name='outgoing')
    target_node = ForeignKey(WorkflowNode, related_name='incoming')

    # Handles
    source_handle = CharField(default='output')
    target_handle = CharField(default='input')

    # Conditional connections
    condition = JSONField()  # Optional condition

    # Data transformation
    transform = JSONField()  # Optional transform
```

**Features**:
- Conditional connections (only activate if condition met)
- Data transformations (JMESPath, JSONPath, templates)
- Multiple handles per node

#### 4. **WorkflowExecution**

Execution history and logging:

```python
class WorkflowExecution(BaseModel):
    workflow = ForeignKey(Workflow)
    status = CharField(choices=['pending', 'running', 'success', 'failed', 'cancelled', 'timeout'])

    # Timing
    started_at = DateTimeField(auto_now_add=True)
    completed_at = DateTimeField(null=True)
    duration_ms = IntegerField(null=True)

    # Trigger info
    triggered_by = ForeignKey(User, null=True)
    trigger_type = CharField(max_length=20)
    trigger_data = JSONField()

    # Data
    input_data = JSONField()
    output_data = JSONField()

    # Errors
    error_message = TextField(blank=True)
    error_traceback = TextField(blank=True)

    # Node logs
    node_logs = JSONField()  # Per-node execution details
```

**Complete audit trail** with per-node execution logs!

#### 5. **WorkflowTemplate**

Pre-built workflow templates:

```python
class WorkflowTemplate(BaseModel):
    name = CharField(max_length=255)
    description = TextField()
    category = CharField(choices=[
        'data_processing',
        'content_generation',
        'monitoring',
        'integration',
        'custom'
    ])

    workflow_data = JSONField()  # Complete workflow config

    # Metadata
    author = ForeignKey(User, null=True)
    is_official = BooleanField(default=False)
    is_public = BooleanField(default=True)
    usage_count = IntegerField(default=0)
    thumbnail_url = URLField(blank=True)
```

**Template marketplace** for sharing workflows!

#### 6. **DataSourceCredential**

Encrypted credentials storage:

```python
class DataSourceCredential(BaseModel):
    user = ForeignKey(User)
    provider = CharField(choices=['google', 'github', 'slack', 'custom'])
    name = CharField(max_length=255)

    # Encrypted credentials
    credentials = JSONField()  # Auto-encrypted on save

    # Status
    is_valid = BooleanField(default=True)
    last_validated_at = DateTimeField(null=True)
    expires_at = DateTimeField(null=True)
```

**Security**: Credentials are automatically encrypted using Fernet encryption!

---

## ğŸ§© Node Types (31 Total)

### Data Source Nodes (7 types)

Fetch data from external sources:

| Node Type | Description | Configuration |
|-----------|-------------|---------------|
| **Google Docs** | Fetch document content via Google Docs API | `document_id` |
| **Google Sheets** | Read spreadsheet data | `spreadsheet_id`, `range` |
| **Webhook Input** | Receive data from HTTP webhooks | Webhook URL |
| **Database Query** | Execute SQL queries | Connection, query |
| **API Request** | Generic HTTP API calls | URL, method, headers, body |
| **File Input** | Read file content | File path/upload |
| **Custom URL** | Fetch data from any URL | URL, method, params |

**Integration**: Direct integration with Google, GitHub, Slack via OAuth!

### Processor Nodes (7 types)

Transform and process data:

| Node Type | Description | Use Case |
|-----------|-------------|----------|
| **LLM Processor** | Process data with LLM | Text generation, analysis, extraction |
| **Data Transform** | Transform data structure | Map, reduce, reshape |
| **Filter** | Filter data by conditions | Remove items, filter arrays |
| **Aggregate** | Aggregate data (sum, avg, etc.) | Statistics, summaries |
| **Split** | Split data into multiple paths | Parallel processing |
| **Merge** | Merge data from multiple sources | Combine results |
| **Custom Code** | Execute custom Python/JavaScript | Advanced transformations |

**Power**: LLM integration allows natural language data processing!

### Output Nodes (7 types)

Send results to destinations:

| Node Type | Description | Configuration |
|-----------|-------------|---------------|
| **Email** | Send email notifications | To, subject, body |
| **Webhook Output** | POST to external webhooks | URL, payload |
| **Database Write** | Insert/update database | Connection, query/data |
| **File Output** | Write to file | Path, format |
| **Slack Message** | Post to Slack channels | Channel, message |
| **API Call** | Call external APIs | URL, method, data |
| **System Notification** | WebOps notifications | User, message, level |

**Flexibility**: Send results anywhere - email, Slack, databases, webhooks!

### Control Flow Nodes (4 types)

Control workflow execution:

| Node Type | Description | Use Case |
|-----------|-------------|----------|
| **Conditional** | Branch based on conditions | If/else logic |
| **Loop** | Iterate over items | Process arrays |
| **Delay** | Wait before continuing | Rate limiting, timing |
| **Error Handler** | Catch and handle errors | Graceful error recovery |

**Smart**: Conditional connections with expression evaluation!

### Agent Integration Nodes (5 types)

AI agent capabilities:

| Node Type | Description | Capability |
|-----------|-------------|------------|
| **Agent Task** | Execute task via AI agent | Complex task automation |
| **Agent Query** | Query agent for information | Natural language Q&A |
| **Agent Memory** | Store/retrieve from agent memory | Persistent context |
| **Agent Decision** | Request decision from agent | Multi-criteria decision making |
| **Agent Learning** | Provide feedback to agent | Continuous learning |

**Unique**: Direct integration with AI agents in `.webops/agents/`!

---

## âš™ï¸ Workflow Execution Engine

### Execution Flow

```python
def execute_workflow(workflow, input_data, triggered_by, trigger_type):
    # 1. Create execution record
    execution = WorkflowExecution.objects.create(
        workflow=workflow,
        status='pending',
        input_data=input_data
    )

    # 2. Get execution order (topological sort)
    execution_order = _get_execution_order(workflow)  # Kahn's algorithm
    if execution_order is None:
        raise ValueError("Workflow contains cycles")

    # 3. Execute nodes in order
    node_data = {'input': input_data}
    for node in execution_order:
        if not node.enabled:
            continue  # Skip disabled nodes

        # Get input from predecessors
        node_input = _get_node_input(node, node_data, workflow)

        # Execute node
        node_result = _execute_node(node, node_input, execution)

        # Handle errors with retry
        if node_result['status'] == 'error':
            if _should_retry_node(node, node_result):
                node_result = _retry_node(node, node_input)  # Exponential backoff

        # Store output
        node_data[node.node_id] = node_result['output']

    # 4. Get final output
    output_data = _get_final_output(execution_order, node_data)

    # 5. Update execution record
    execution.status = 'success'
    execution.output_data = output_data
    execution.save()

    # 6. Update workflow statistics (atomic!)
    workflow.total_executions = F('total_executions') + 1
    workflow.successful_executions = F('successful_executions') + 1
    workflow.save()
```

### Key Features

#### 1. **Topological Sorting**

Uses **Kahn's algorithm** with `collections.deque` for O(1) operations:

```python
def _get_execution_order(workflow):
    nodes = list(workflow.nodes.all())
    connections = list(workflow.connections.all())

    # Build adjacency list and in-degree
    adjacency = {node.node_id: [] for node in nodes}
    in_degree = {node.node_id: 0 for node in nodes}

    for conn in connections:
        adjacency[conn.source_node.node_id].append(conn.target_node.node_id)
        in_degree[conn.target_node.node_id] += 1

    # Kahn's algorithm
    queue = deque([node_id for node_id, degree in in_degree.items() if degree == 0])
    result = []

    while queue:
        node_id = queue.popleft()
        result.append(node_id)

        for neighbor in adjacency[node_id]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)

    # Check for cycles
    if len(result) != len(nodes):
        cycle_nodes = [node_id for node_id in in_degree if in_degree[node_id] > 0]
        logger.error(f"Workflow contains cycles: {cycle_nodes}")
        return None

    return [node_map[node_id] for node_id in result]
```

**Performance**: O(V + E) complexity - optimal for DAG traversal!

#### 2. **Data Flow Management**

Intelligent data merging from multiple predecessors:

```python
def _get_node_input(node, node_data, workflow):
    incoming = [conn for conn in workflow.connections.all()
                if conn.target_node_id == node.id]

    if not incoming:
        return node_data.get('input', {})  # Root node

    merged_input = {}
    for conn in incoming:
        source_data = node_data.get(conn.source_node.node_id, {})

        # Apply transformation
        if conn.transform:
            source_data = _apply_transformation(source_data, conn.transform)

        # Check condition
        if conn.condition:
            if not _evaluate_condition(source_data, conn.condition):
                continue  # Skip this connection

        # Merge data
        if isinstance(source_data, dict):
            merged_input.update(source_data)
        else:
            merged_input[conn.source_handle] = source_data

    return merged_input
```

**Smart Merging**: Supports conditional connections and data transformations!

#### 3. **Error Handling & Retry**

Exponential backoff with jitter:

```python
def _should_retry_node(node, node_result, execution):
    if not node.retry_on_failure:
        return False

    retry_count = execution.node_logs.count()
    if retry_count >= node.max_retries:
        return False

    error_message = node_result.get('error', '').lower()

    # Don't retry configuration errors
    non_retryable = ['not configured', 'not found', 'permission denied',
                     'authentication', 'invalid', 'malformed']
    if any(err in error_message for err in non_retryable):
        return False

    # Retry network errors, timeouts, etc.
    retryable = ['timeout', 'connection', 'network', 'temporary', 'rate limit']
    if any(err in error_message for err in retryable):
        return True

    return True  # Default to retry

def _retry_node(node, input_data, execution):
    import time, random

    retry_count = len([log for log in execution.node_logs
                       if log.get('node_id') == node.node_id])
    base_delay = 2  # seconds
    max_delay = 30  # seconds
    delay = min(base_delay * (2 ** retry_count), max_delay)

    # Add jitter to prevent thundering herd
    jitter = random.uniform(0, 0.1 * delay)
    time.sleep(delay + jitter)

    logger.info(f"Retrying node {node.label} (attempt {retry_count + 1}) "
                f"after {delay:.2f}s delay")

    return _execute_node(node, input_data, execution)
```

**Intelligent**: Only retries transient errors, not configuration errors!

#### 4. **Safe Expression Evaluation**

No `eval()` - uses AST parsing for safety:

```python
def _safe_eval_expression(expression, data):
    import ast
    import operator

    operators = {
        ast.Eq: operator.eq,
        ast.NotEq: operator.ne,
        ast.Lt: operator.lt,
        ast.GtE: operator.ge,
        ast.And: lambda a, b: a and b,
        ast.Or: lambda a, b: a or b,
        ast.In: lambda a, b: a in b,
    }

    def _eval(node):
        if isinstance(node, ast.Compare):
            left = _eval(node.left)
            for op, right in zip(node.ops, node.comparators):
                right_val = _eval(right)
                if not operators[type(op)](left, right_val):
                    return False
                left = right_val
            return True

        elif isinstance(node, ast.Name):
            if node.id == 'data':
                return data
            raise ValueError(f"Unknown variable: {node.id}")

        elif isinstance(node, ast.Constant):
            return node.value

        elif isinstance(node, ast.Attribute):
            # Allow: data.field_name
            if isinstance(node.value, ast.Name) and node.value.id == 'data':
                return data.get(node.attr) if isinstance(data, dict) else getattr(data, node.attr, None)

        elif isinstance(node, ast.Subscript):
            # Allow: data['key'] or data[0]
            if isinstance(node.value, ast.Name) and node.value.id == 'data':
                key = node.slice.value if isinstance(node.slice, ast.Constant) else _eval(node.slice)
                if isinstance(data, dict):
                    return data.get(key)
                elif isinstance(data, (list, tuple)):
                    return data[key] if isinstance(key, int) else None

    tree = ast.parse(expression, mode='eval')
    return _eval(tree.body)
```

**Security**: No arbitrary code execution - only safe operations allowed!

---

## ğŸ”Œ Agent Integration

### AgentBridge Class

Connects automation workflows with AI agents:

```python
class AgentBridge:
    """Bridge between automation workflows and AI agents."""

    def execute_task(self, agent_id, task_description, task_params):
        """Execute task using AI agent."""
        agent = self._get_agent(agent_id)

        # Run async agent task synchronously
        loop = asyncio.new_event_loop()
        result = loop.run_until_complete(
            self._async_execute_task(agent, task_description, task_params)
        )
        loop.close()

        return result

    def process_query(self, agent_id, query, context, expected_format):
        """Query agent for information."""
        # Natural language Q&A with agents

    def store_memory(self, agent_id, memory_type, content):
        """Store in agent memory."""
        # Persistent context across workflows

    def make_decision(self, agent_id, context, options, criteria):
        """Request decision from agent."""
        # Multi-criteria decision making

    def process_learning(self, agent_id, feedback_type, feedback_data):
        """Provide learning feedback."""
        # Continuous improvement
```

**Capabilities**:
- âœ… Execute complex tasks via natural language
- âœ… Query agents for information
- âœ… Store and retrieve memories
- âœ… Request multi-criteria decisions
- âœ… Provide learning feedback

**Mock Mode**: Gracefully falls back to mock responses when agent system unavailable!

---

## ğŸ›¡ï¸ Security Features

### 1. **Credential Encryption**

Automatic encryption of sensitive data:

```python
def save(self, *args, **kwargs):
    """Override save to encrypt credentials before saving."""
    if self.credentials:
        self.credentials = self._encrypt_credentials(self.credentials)
    super().save(*args, **kwargs)

def _encrypt_credentials(self, credentials):
    """Encrypt sensitive fields."""
    from apps.core.utils import encrypt_password

    sensitive_fields = [
        'api_key', 'token', 'access_token', 'refresh_token',
        'private_key', 'password', 'secret', 'client_secret'
    ]

    encrypted = {}
    for key, value in credentials.items():
        if any(field in key.lower() for field in sensitive_fields):
            encrypted[key] = encrypt_password(value)  # Fernet encryption
        else:
            encrypted[key] = value

    return encrypted
```

**Security**: Uses Fernet symmetric encryption from `cryptography` library!

### 2. **SSRF Protection**

URL validation prevents internal network access:

```python
def _validate_url(self, url):
    """Validate URL to prevent SSRF attacks."""
    from urllib.parse import urlparse
    import ipaddress, socket

    parsed = urlparse(url)

    # Only allow HTTP/HTTPS
    if parsed.scheme not in ('http', 'https'):
        raise ValueError(f"Scheme '{parsed.scheme}' not allowed")

    # Resolve hostname and check if private/localhost
    ip = socket.gethostbyname(parsed.hostname)
    ip_obj = ipaddress.ip_address(ip)

    if ip_obj.is_private or ip_obj.is_loopback:
        raise ValueError(f"Access to internal network '{ip}' not allowed")

    # Block AWS metadata, GCP metadata, etc.
    blocked_ranges = [
        ipaddress.ip_network('169.254.169.254/32'),  # AWS
    ]

    for blocked in blocked_ranges:
        if ip_obj in blocked:
            raise ValueError(f"Access to '{ip}' not allowed")

    # Block hostname patterns
    blocked_hostnames = ['localhost', 'metadata.google.internal']
    if parsed.hostname.lower() in blocked_hostnames:
        raise ValueError(f"Access to '{parsed.hostname}' not allowed")
```

**Protection**: Blocks access to:
- âŒ Localhost (127.0.0.1)
- âŒ Private networks (10.0.0.0/8, 192.168.0.0/16)
- âŒ Cloud metadata endpoints (169.254.169.254)
- âŒ Link-local addresses

### 3. **Safe Expression Evaluation**

No `eval()` - only AST parsing:
- âœ… Whitelist of operators (eq, ne, lt, gt, and, or, in)
- âœ… Only `data` variable accessible
- âœ… Attribute and subscript access allowed
- âŒ No arbitrary function calls
- âŒ No imports
- âŒ No file operations

---

## ğŸ¨ Visual Workflow Builder

### React Flow Integration

Uses **React Flow** for the visual canvas:

```javascript
// Workflow builder features:
- Drag & drop nodes
- Visual connections
- Auto-layout
- Zoom/pan canvas
- Minimap
- Node configuration panels
- Real-time validation
```

### Canvas Data Structure

```json
{
  "nodes": [
    {
      "id": "node-1",
      "type": "google_docs",
      "label": "Fetch Document",
      "position": {"x": 100, "y": 100},
      "config": {
        "document_id": "abc123"
      }
    },
    {
      "id": "node-2",
      "type": "llm",
      "label": "Summarize",
      "position": {"x": 300, "y": 100},
      "config": {
        "model": "gpt-4",
        "prompt": "Summarize: {content}"
      }
    },
    {
      "id": "node-3",
      "type": "email",
      "label": "Send Summary",
      "position": {"x": 500, "y": 100},
      "config": {
        "to": "user@example.com",
        "subject": "Document Summary",
        "body": "{summary}"
      }
    }
  ],
  "connections": [
    {
      "source": "node-1",
      "target": "node-2",
      "sourceHandle": "output",
      "targetHandle": "input"
    },
    {
      "source": "node-2",
      "target": "node-3",
      "sourceHandle": "output",
      "targetHandle": "input"
    }
  ]
}
```

---

## ğŸ“‹ Use Cases

### 1. **Document Processing Pipeline**

```
[Google Docs] â†’ [LLM Summarizer] â†’ [Email] + [Database Write]
```

**Workflow**:
1. Fetch document from Google Docs
2. Summarize with LLM
3. Email summary to team
4. Store in database for records

**Real-world**: Auto-summarize meeting notes and distribute!

### 2. **API Monitoring & Alerting**

```
[Schedule: */5 * * * *] â†’ [API Request] â†’ [Condition] â†’ [Slack Alert]
```

**Workflow**:
1. Run every 5 minutes (cron: `*/5 * * * *`)
2. Call health check API
3. Check if response time > 1000ms
4. Send Slack alert if slow

**Real-world**: Monitor deployment health!

### 3. **Data ETL Pipeline**

```
[Database Query] â†’ [Transform] â†’ [Filter] â†’ [Aggregate] â†’ [Webhook Output]
```

**Workflow**:
1. Extract data from PostgreSQL
2. Transform data structure
3. Filter invalid records
4. Aggregate statistics
5. POST to analytics webhook

**Real-world**: Daily deployment metrics!

### 4. **Content Generation**

```
[Webhook Input] â†’ [Agent Query] â†’ [LLM Processor] â†’ [File Output]
```

**Workflow**:
1. Receive content request via webhook
2. Query agent for context
3. Generate content with LLM
4. Save to file

**Real-world**: Auto-generate documentation!

### 5. **Intelligent Decision Making**

```
[Database Query] â†’ [Agent Decision] â†’ [Condition] â†’ [Email] or [Webhook]
```

**Workflow**:
1. Fetch deployment metrics
2. Agent decides: scale up/down/maintain
3. If scale action needed â†’ notify ops
4. Else â†’ log decision

**Real-world**: Auto-scaling decisions!

---

## ğŸ”„ Data Transformations

### Supported Transform Types

#### 1. **JMESPath**

```json
{
  "type": "jmespath",
  "query": "items[?price > `100`].{name: name, price: price}"
}
```

**Use case**: Filter and reshape complex JSON

#### 2. **JSONPath**

```json
{
  "type": "jsonpath",
  "query": "$.data[*].value"
}
```

**Use case**: Extract values from nested structures

#### 3. **Template**

```json
{
  "type": "template",
  "template": "Hello ${name}, your order #${order_id} is ${status}"
}
```

**Use case**: Format strings with variables

---

## ğŸ“Š Statistics & Monitoring

### Workflow Statistics

Tracked per workflow:
- âœ… Total executions
- âœ… Successful executions
- âœ… Failed executions
- âœ… Last executed time
- âœ… Average duration (ms)

### Execution Logs

Per execution:
- âœ… Complete execution trace
- âœ… Per-node execution logs
- âœ… Error messages and tracebacks
- âœ… Input/output data
- âœ… Duration metrics

### Performance Optimization

```python
# Atomic statistics updates (no race conditions!)
with transaction.atomic():
    workflow = Workflow.objects.select_for_update().get(pk=workflow.pk)
    workflow.total_executions = F('total_executions') + 1
    workflow.successful_executions = F('successful_executions') + 1
    workflow.save()
```

**Optimization**: Uses `F()` expressions for database-level atomic operations!

---

## ğŸ­ Comparison with Other Platforms

| Feature | WebOps Automation | Zapier | n8n | Airflow |
|---------|-------------------|--------|-----|---------|
| **Visual Builder** | âœ… React Flow | âœ… | âœ… | âŒ (Code-based) |
| **Self-hosted** | âœ… | âŒ | âœ… | âœ… |
| **AI Agent Integration** | âœ… Unique! | âŒ | âŒ | âŒ |
| **LLM Nodes** | âœ… | âš ï¸  Limited | âš ï¸  Limited | âŒ |
| **Google Docs Integration** | âœ… | âœ… | âœ… | âŒ |
| **Conditional Connections** | âœ… | âœ… | âœ… | âš ï¸  |
| **Data Transformations** | âœ… JMESPath, JSONPath | âš ï¸  Limited | âœ… | âœ… |
| **Retry Logic** | âœ… Exponential backoff | âœ… | âœ… | âœ… |
| **SSRF Protection** | âœ… | âœ… | âš ï¸  | âš ï¸  |
| **Credential Encryption** | âœ… Fernet | âœ… | âœ… | âš ï¸  |
| **Templates** | âœ… | âœ… | âœ… | âŒ |
| **Schedule Triggers** | âœ… Cron | âœ… | âœ… | âœ… |
| **Webhook Triggers** | âœ… | âœ… | âœ… | âš ï¸  |
| **Cost** | Free (self-hosted) | $$$ Monthly | Free | Free |

### WebOps Unique Features

1. **âœ¨ AI Agent Integration**: Direct integration with agents in `.webops/agents/`
2. **âœ¨ Deployment Integration**: Access to deployment data and operations
3. **âœ¨ Self-hosted**: Complete control, no external dependencies
4. **âœ¨ Security First**: SSRF protection, credential encryption, safe eval
5. **âœ¨ Performance**: Optimized with F() expressions, topological sorting

---

## ğŸš€ Technical Highlights

### 1. **Graph Algorithm**

- **Kahn's Algorithm** for topological sort
- **O(V + E)** complexity - optimal
- **Cycle detection** with detailed error messages
- **deque** for O(1) queue operations

### 2. **Database Optimization**

- **Atomic updates** with `F()` expressions
- **select_for_update()** to prevent race conditions
- **Prefetch related** to reduce queries
- **Indexes** on workflow_id, status, started_at

### 3. **Security**

- **Fernet encryption** for credentials
- **SSRF protection** for URL nodes
- **AST parsing** for safe expression evaluation
- **No eval()** - zero arbitrary code execution

### 4. **Async/Sync Bridge**

- Synchronous workflow engine
- Async agent system
- **asyncio.run_until_complete()** for bridging
- Graceful fallback to mock responses

---

## ğŸ“ˆ Statistics

| Metric | Count |
|--------|-------|
| **Total Node Types** | 31 |
| **Data Sources** | 7 |
| **Processors** | 7 |
| **Outputs** | 7 |
| **Control Flow** | 4 |
| **Agent Nodes** | 5 |
| **Database Models** | 6 |
| **Node Executors** | 20+ implemented |
| **Security Features** | 3 (SSRF, encryption, safe eval) |
| **Transform Types** | 3 (JMESPath, JSONPath, template) |
| **Lines of Code** | ~3,000+ |

---

## ğŸ¯ Future Enhancements

### Planned Features

1. **More Integrations**
   - GitHub Actions
   - GitLab CI/CD
   - Bitbucket
   - Jira
   - Trello

2. **Advanced Control Flow**
   - Parallel execution
   - Sub-workflows
   - Dynamic branching

3. **Enhanced Monitoring**
   - Real-time execution view
   - Performance metrics dashboard
   - Alert rules

4. **Template Marketplace**
   - Community templates
   - Template categories
   - Rating & reviews

5. **Versioning**
   - Workflow version control
   - Rollback to previous versions
   - Change history

---

## ğŸ‰ Conclusion

WebOps Automation System provides:

âœ… **Visual workflow builder** - Drag & drop interface
âœ… **31 node types** - Data sources, processors, outputs, control flow
âœ… **AI agent integration** - Unique capability!
âœ… **Enterprise security** - SSRF protection, encryption, safe eval
âœ… **Production-ready** - Retry logic, error handling, monitoring
âœ… **Self-hosted** - Complete control, no external dependencies
âœ… **Performance optimized** - Atomic updates, efficient algorithms
âœ… **Extensible** - Easy to add new node types

**The power of Zapier + n8n, integrated into your deployment platform, with AI agents!**

---

**Built for automation enthusiasts, powered by graph algorithms, secured by design.**

*WebOps - Enterprise-Grade Auto-Deployment Platform*
